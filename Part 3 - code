pip install datasets sentence_transformers cohere numpy pinecone tqdm

from datasets import load_dataset
from sentence_transformers import SentenceTransformer,util
import cohere
import numpy as np
import pinecone
from tqdm import tqdm
import os
import warnings
from IPython.display import display
warnings.filterwarnings("ignore")

COHERE_API_KEY = "INdVNJcXJYO5nNQHovgyn966o25uXp7ncltupjhQ"
PINECONE_API_KEY = "46c01a13-e885-432d-9c5f-5a96248cbff2"

# Constants
DATASET_NAME = 'squad'
COHERE_API_KEY = "INdVNJcXJYO5nNQHovgyn966o25uXp7ncltupjhQ"
PINECONE_API_KEY = "46c01a13-e885-432d-9c5f-5a96248cbff2"
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
INDEX_NAME = "squad-index"


# Initialize SentenceTransformer with GPU support
model = SentenceTransformer(EMBEDDING_MODEL)
co = cohere.Client(api_key=COHERE_API_KEY)

# Function to preprocess documents
def preprocess_document(text):
    """Preprocesses a document text by stripping and lowercasing."""
    return text.strip().lower()

# Function to create Pinecone index
def create_pinecone_index(api_key, index_name, dimension, metric='cosine'):
    """
    Creates or retrieves a Pinecone index.
    Args:
        api_key: Pinecone API key.
        index_name: Name of the index to create or retrieve.
        dimension: Dimensionality of the embeddings.
        metric: Similarity metric for the index.
    Returns:
        Pinecone index object.
    """
    pc = pinecone.Pinecone(api_key=api_key)
    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=dimension,
            metric=metric,
            spec=pinecone.ServerlessSpec(
                cloud='aws',
                region='us-east-1'
            )
        )
    return pc.Index(index_name)

# Function to upsert vectors into Pinecone index
def upsert_vectors(index, embeddings, dataset, text_field='context', batch_size=128):
    """
    Upserts embeddings and corresponding metadata into a Pinecone index.
    Args:
        index: Pinecone index object.
        embeddings: NumPy array of embeddings.
        dataset: Dataset containing the metadata.
        text_field: Field name containing the text data.
        batch_size: Batch size for upsert operations.
    Returns:
        Updated Pinecone index object.
    """
    print("Upserting the embeddings to the Pinecone index...")
    shape = embeddings.shape

    ids = [str(i) for i in range(shape[0])]
    meta = [{text_field: preprocess_document(text)} for text in dataset[text_field]]

    to_upsert = list(zip(ids, embeddings, meta))

    for i in tqdm(range(0, shape[0], batch_size)):
        i_end = min(i + batch_size, shape[0])
        index.upsert(vectors=to_upsert[i:i_end])
    return index

# Function to augment prompt with retrieved documents
def augment_prompt(query, model, index, top_k=3):
    """
    Augments the query prompt with retrieved documents from the Pinecone index.
    Args:
        query: Query string.
        model: SentenceTransformer model for encoding queries.
        index: Pinecone index object.
        top_k: Number of top documents to retrieve.
    Returns:
        Augmented prompt and retrieved source knowledge as strings.
    """
    query_embedding = model.encode(query).tolist()
    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)['matches']
    text_matches = [match['metadata']['context'] for match in results]
    source_knowledge = "\n\n".join(text_matches)
    augmented_prompt = f"""Using the contexts below, answer the query.
    Contexts:
    {source_knowledge}
    If the answer is not included in the source knowledge - say that you don't know.
    Query: {query}"""
    return augmented_prompt, source_knowledge

# Function to load and embed dataset for RAG pipeline
def load_and_embed_dataset(dataset_name='squad', split='train', model_name='all-MiniLM-L6-v2', text_field='context', rec_num=87600):
    """
    Loads and embeds a dataset using SentenceTransformer.
    Args:
        dataset_name: Name of the dataset.
        split: Split of the dataset to load.
        model_name: Name of the SentenceTransformer model to use.
        text_field: Field name containing the text data.
        rec_num: Number of records to process.
    Returns:
        Loaded dataset and corresponding embeddings.
    """
    print("Loading and embedding the dataset...")

    # Load the dataset
    dataset = load_dataset(dataset_name, split=split)

    # Embed the first `rec_num` rows of the dataset
    embeddings = model.encode(dataset[text_field][:rec_num])

    print("Done!")
    return dataset, embeddings


### Standard QA Model ###
def standard_qa_model(query, questions, answers):
    """
    Performs standard question answering using a list of questions and answers.
    Args:
        query: Query string.
        questions: List of questions from the dataset.
        answers: List of corresponding answers from the dataset.
    Returns:
        Answer string if found, otherwise "Answer not found.".
    """
    for idx, q in enumerate(questions):
        if query.lower() in q.lower():
            return answers[idx]['text']
    return "Answer not found."

# Load SQuAD dataset
dataset = load_dataset(DATASET_NAME, split='train')
questions = dataset['question']
answers = dataset['answers']

### RAG Pipeline ###
def run_rag_pipeline(dataset_name='squad', split='train', model_name='all-MiniLM-L6-v2', text_field='context', rec_num=87600):
    """
    Runs the Retrieval-Augmented Generation (RAG) pipeline for question answering.
    Args:
        dataset_name: Name of the dataset.
        split: Split of the dataset to load.
        model_name: Name of the SentenceTransformer model to use.
        text_field: Field name containing the text data.
        rec_num: Number of records to process.
    Returns:
        RAG pipeline response as a string.
    """
    # Load and embed dataset
    dataset, document_embeddings = load_and_embed_dataset(dataset_name, split, model_name, text_field, rec_num)
    
    # Create or retrieve Pinecone index
    index = create_pinecone_index(PINECONE_API_KEY, INDEX_NAME, document_embeddings.shape[1])
    
    # Upsert embeddings into Pinecone index
    index = upsert_vectors(index, document_embeddings, dataset)
    
    return index

index = run_rag_pipeline()   

# Example query
query_list = [
    "Who was the first woman to win a Nobel Prize?",
    "How has the COVID-19 pandemic impacted global economic trends?",
    "What year did Albert Einstein publish his theory of general relativity?"
]

for query in query_list:
    print(f'{query}\n')
    

    # Run Standard QA Model
    standard_response = standard_qa_model(query, questions, answers)
    print(f"Standard QA Model Response: {standard_response}\n")

    # Run RAG pipeline
    # Augment prompt with retrieved documents
    augmented_prompt, _ = augment_prompt(query, model, index)  
    # Generate answer using RAG pipeline
    rag_response = co.chat(model='command-r-plus', message=augmented_prompt).text
    print(f"RAG Pipeline Response: {rag_response}\n")
    print("===============================================================\n")


